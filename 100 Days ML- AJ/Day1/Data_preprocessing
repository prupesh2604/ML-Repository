#Step :1 Importing libraries needed
import numpy as np
import pandas as pd
import os 
from sklearn.preprocessing import Imputer      #to handle missed data ,we fill with mean, median of that column 
from sklearn.preprocessing import LabelEncoder, OneHotEncoder   #to handle categorical data
from sklearn.cross_validation import train_test_split     #to split datasets into training sets and test sets
from sklearn.preprocessing import StandardScaler     #

#Step :2 Importing the dataset
dataset = pd.read_csv(r"D:\100_Days_of_ML\Datasets\SampleData.csv")
#Matrix for independent variables 
X = dataset.iloc[ : , :-1].values
#Vector for dependent variables 
Y = dataset.iloc[ : , 3].values

#Step :3 handling the missed data
imputer = Imputer(missing_values="NaN" , strategy= "mean" , axis=0)
imputer = imputer.fit(X[ : , 1:3])
X[ : , 1:3] = imputer.transform(X[ : ,1:3])


#Step :4 Encoding Categorical data
labelencoder_X = LabelEncoder()
X[: , 0] = labelencoder_X.fit_transform(X[ : , 0])
#creating dummy variable
onehotencoder = OneHotEncoder(categorical_features = [0])
X = onehotencoder.fit_transform(X).toarray()
labelencoder_Y = LabelEncoder()
Y = labelencoder_Y.fit_transform(Y)
 
 
#Step :5 Splitting Dataset into test set(test performance fo trained model) and training set (for training the model)
X_train, X_test, Y_train, Y_test = train_test_split( X,Y,test_size=0.2 , random_state = 0)


#Step :6 Feature Scaling
sc_X = StandardScaler()
X_train = sc_X.fit_transform(X_train)
X_test = sc_X.fit_transform(X_test)
